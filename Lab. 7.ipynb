{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Laboratorium 7\n",
    "\n",
    "Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import random"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dołączenie bibliotek do obsługi sieci neuronowych"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ],
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zadanie 1 - Actor-Critic\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n",
    "    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n",
    "    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n",
    "Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n",
    "\\end{equation*}\n",
    "Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n",
    "\\end{equation*}\n",
    "gdzie:\n",
    "\\begin{equation*}\n",
    "    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n",
    "\\end{equation*}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, lr, state_shape, n_actions, fc1, fc2):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_shape, fc1)\n",
    "        self.fc2 = nn.Linear(fc1, fc2)\n",
    "        self.output = nn.Linear(fc2, n_actions)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0') if T.cuda.is_available() else T.device('cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = F.relu(self.fc2(state))\n",
    "        probs = self.softmax(self.output(state))\n",
    "\n",
    "        return probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, lr, state_shape, fc1, fc2):\n",
    "        super(CriticModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_shape, fc1)\n",
    "        self.fc2 = nn.Linear(fc1, fc2)\n",
    "        self.output = nn.Linear(fc2, 1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0') if T.cuda.is_available() else T.device('cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = F.relu(self.fc2(state))\n",
    "        probs = self.output(state)\n",
    "\n",
    "        return probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class ACActor:\n",
    "    def __init__(self, state_shape, n_actions, fc1: int = 128, fc2: int = 128, gamma: float = 0.99, alpha: float = 0.001, beta: float = 0.001):\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.actor = ActorNetwork(alpha, state_shape, n_actions, fc1, fc2).double()\n",
    "        self.critic = CriticModel(beta, state_shape, fc1, fc2).double()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = T.tensor(state).to(self.actor.device)\n",
    "        probs = self.actor.forward(state).cpu().detach().numpy()\n",
    "        action = np.random.choice(self.n_actions, p=probs)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, _state, done):\n",
    "\n",
    "        state = T.tensor(state).to(self.actor.device)\n",
    "        _state = T.tensor(_state).to(self.actor.device)\n",
    "        reward = T.tensor(reward).to(self.actor.device)\n",
    "        action = T.tensor(action).to(self.actor.device)\n",
    "\n",
    "        probs = self.actor.forward(state)\n",
    "        cat = Categorical(probs)\n",
    "        log_action = cat.log_prob(action)\n",
    "        v = self.critic.forward(state)\n",
    "        _v = self.critic.forward(_state)\n",
    "\n",
    "        delta = reward + self.gamma * _v * (1-done) - v\n",
    "\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss = -delta * log_action\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = delta ** 2\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()"
   ],
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "alpha_learning_rate = 0.0001\n",
    "beta_learning_rate = 0.0005\n"
   ],
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Czas nauczyć agenta gry w środowisku *CartPool*:"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "agent = ACActor(state_size, action_size, alpha=alpha_learning_rate, beta=beta_learning_rate)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    score_history = []\n",
    "\n",
    "    for i in range(100):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state.astype(np.float32)\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            _state, reward, done, _ = env.step(action)\n",
    "            _state.astype(np.float32)\n",
    "            agent.learn(state, action, reward, _state, done)\n",
    "            state = _state\n",
    "            score += reward\n",
    "        score_history.append(score)\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n",
    "\n",
    "    if np.mean(score_history) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ],
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:27.880\n",
      "mean reward:61.920\n",
      "mean reward:132.760\n",
      "mean reward:263.920\n",
      "mean reward:239.360\n",
      "mean reward:368.250\n",
      "You Win!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
